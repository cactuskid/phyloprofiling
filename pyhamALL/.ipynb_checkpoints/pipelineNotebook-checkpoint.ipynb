{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurent/anaconda3/lib/python3.6/site-packages/tables/leaf.py:396: PerformanceWarning: The Leaf ``/Protein/_i_Entries/OmaHOG/sorted`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
      "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
      "I/O.  You may want to reduce the rowsize by trimming the value of\n",
      "dimensions that are orthogonal (and preferably close) to the *main*\n",
      "dimension of this leave.  Alternatively, in case you have specified a\n",
      "very small/large chunksize, you may want to increase/decrease it.\n",
      "  PerformanceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "524\n",
      "524\n",
      "524\n",
      "524\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pyham\n",
    "from pyoma.browser import db \n",
    "import numpy as np\n",
    "from tables import *\n",
    "import re\n",
    "from ete3 import Tree\n",
    "import pickle\n",
    "import tempfile\n",
    "import functools\n",
    "import config\n",
    "\n",
    "import pyhamPipeline\n",
    "import profileGen\n",
    "import format_files\n",
    "\n",
    "from datasketch import MinHashLSH\n",
    "\n",
    "import h5py\n",
    "\n",
    "h5file = h5py.File(config.omadirLaurent + 'data1', 'a')\n",
    "\n",
    "dataset_names = ['fam', 'duplication', 'gain', 'loss', 'presence']\n",
    "list(h5file.keys())\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    if dataset_name not in list(h5file.keys()):\n",
    "        dataset = h5file.create_dataset(dataset_name, (0,0), maxshape=(None, None), dtype = 'int32')\n",
    "\n",
    "dsets = {}\n",
    "for dataset_name in list(h5file.keys()):\n",
    "    dsets[dataset_name] = h5file[dataset_name]\n",
    "\n",
    "#open up OMA\n",
    "h5file = open_file(config.omadirLaurent + 'OmaServer.h5', mode=\"r\") \n",
    "#setup db objects\n",
    "dbObj = db.Database(h5file)\n",
    "omaIdObj = db.OmaIdMapper(dbObj)\n",
    "\n",
    "# corrects species tree and replacement dictionary for orthoXML files\n",
    "\n",
    "dic, tree = format_files.create_species_tree(h5file, omaIdObj)\n",
    "chunksize = 3\n",
    "\n",
    "#temporary, \n",
    "mat_list = []\n",
    "dico_list = []\n",
    "lsh = MinHashLSH()\n",
    "\n",
    "#load Fam\n",
    "for i,fam in enumerate(pyhamPipeline.yieldFamilies(h5file)):\n",
    "#generate pyham object\n",
    "#     try: \n",
    "    \n",
    "    if fam > 0 and fam < 5:\n",
    "        print(fam)\n",
    "        #generate treemap profile\n",
    "        treemap_fam = pyhamPipeline.get_hamTree(fam, dbObj, tree, dic)\n",
    "        # generate matrix of hash\n",
    "        hashesDic = profileGen.Tree2Hashes(treemap_fam, fam, lsh)\n",
    "\n",
    "        if i == 0:\n",
    "            for dataset in dsets:\n",
    "                if dataset == 'fam':\n",
    "                    dsets[dataset].resize((chunksize, 1 ))\n",
    "                else: \n",
    "                    dsets[dataset].resize((chunksize, np.asarray(hashesDic[dataset]).shape[0] ))\n",
    "            \n",
    "        if i % chunksize == 0 and i != 0:\n",
    "            for dataset in dsets:\n",
    "                if dataset == 'fam':\n",
    "                    dsets[dataset].resize((i+chunksize, 1 ))\n",
    "                else: \n",
    "                    dsets[dataset].resize((i+chunksize, np.asarray(hashesDic[dataset]).shape[0] ))\n",
    "\n",
    "\n",
    "        for dset in dsets:\n",
    "            if dset == 'fam':\n",
    "                dsets[dset][i,:] = fam\n",
    "            else:\n",
    "                dsets[dset][i,:] = hashesDic[dset]\n",
    "            \n",
    "h5file.close()\n",
    "#         dico_list.append(dico)\n",
    "#         # generate taxa index\n",
    "#         taxaIndex, taxaIndexReverse = profileGen.generateTaxaIndex(tree)\n",
    "\n",
    "#         # generate matrix of 1 and 0 for each biological event\n",
    "#         mat = profileGen.Tree2mat(treemap_fam, taxaIndex)\n",
    "#         mat_list.append(mat)\n",
    "\n",
    "#     except:\n",
    "#         print(\"breaking at {}\".format(fam))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dsets['fam'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   0,   0, ..., 255, 255, 255],\n",
       "       [  1,   0,   0, ..., 255, 255, 255],\n",
       "       [  1,   0,   0, ..., 255, 255, 255],\n",
       "       [  1,   0,   0, ..., 133, 140,  55],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dsets['duplication'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['datatest']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5file = h5py.File(config.omadirLaurent + 'data1', 'a')\n",
    "\n",
    "list(h5file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-9dd8862e7221>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-9dd8862e7221>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    dset = f.create_dataset(\"test\", (5,10), dtype=([(\"fam\":\"i\"), (\"duplication\":\"S128\")]))\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File('hogTest.hdf5','w')\n",
    "dset = f.create_dataset(\"test\", (5,10), dtype=([(\"fam\":\"i\"), (\"duplication\":\"S128\")]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-61cdd2f5cda0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-61cdd2f5cda0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    dset = f.create_dataset(\"test\", (5,10), dtype([ (\"field1\": \"i\"), (\"field2\": \"f\") ]))\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dset = f.create_dataset(\"test\", (5,10), dtype([ (\"field1\": \"i\"), (\"field2\": \"f\") ]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
