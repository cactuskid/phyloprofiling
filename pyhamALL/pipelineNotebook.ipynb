{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurent/anaconda3/lib/python3.6/site-packages/tables/leaf.py:396: PerformanceWarning: The Leaf ``/Protein/_i_Entries/OmaHOG/sorted`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
      "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
      "I/O.  You may want to reduce the rowsize by trimming the value of\n",
      "dimensions that are orthogonal (and preferably close) to the *main*\n",
      "dimension of this leave.  Alternatively, in case you have specified a\n",
      "very small/large chunksize, you may want to increase/decrease it.\n",
      "  PerformanceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "524\n",
      "524\n",
      "524\n",
      "524\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pyham\n",
    "from pyoma.browser import db \n",
    "import numpy as np\n",
    "from tables import *\n",
    "import re\n",
    "from ete3 import Tree\n",
    "import pickle\n",
    "import tempfile\n",
    "import functools\n",
    "import config\n",
    "\n",
    "import pyhamPipeline\n",
    "import profileGen\n",
    "import format_files\n",
    "\n",
    "from datasketch import MinHashLSH\n",
    "\n",
    "import h5py\n",
    "\n",
    "h5file = h5py.File(config.omadirLaurent + 'data1', 'a')\n",
    "\n",
    "dataset_names = ['fam', 'duplication', 'gain', 'loss', 'presence']\n",
    "list(h5file.keys())\n",
    "\n",
    "for dataset_name in dataset_names:\n",
    "    if dataset_name not in list(h5file.keys()):\n",
    "        dataset = h5file.create_dataset(dataset_name, (0,0), maxshape=(None, None), dtype = 'int32')\n",
    "\n",
    "dsets = {}\n",
    "for dataset_name in list(h5file.keys()):\n",
    "    dsets[dataset_name] = h5file[dataset_name]\n",
    "\n",
    "#open up OMA\n",
    "h5file = open_file(config.omadirLaurent + 'OmaServer.h5', mode=\"r\") \n",
    "#setup db objects\n",
    "dbObj = db.Database(h5file)\n",
    "omaIdObj = db.OmaIdMapper(dbObj)\n",
    "\n",
    "# corrects species tree and replacement dictionary for orthoXML files\n",
    "\n",
    "dic, tree = format_files.create_species_tree(h5file, omaIdObj)\n",
    "chunksize = 3\n",
    "\n",
    "#temporary, \n",
    "mat_list = []\n",
    "dico_list = []\n",
    "lsh = MinHashLSH()\n",
    "\n",
    "#load Fam\n",
    "for i,fam in enumerate(pyhamPipeline.yieldFamilies(h5file)):\n",
    "#generate pyham object\n",
    "#     try: \n",
    "    \n",
    "    if fam > 0 and fam < 5:\n",
    "        print(fam)\n",
    "        #generate treemap profile\n",
    "        treemap_fam = pyhamPipeline.get_hamTree(fam, dbObj, tree, dic)\n",
    "        # generate matrix of hash\n",
    "        hashesDic = profileGen.Tree2Hashes(treemap_fam, fam, lsh)\n",
    "\n",
    "        if i == 0:\n",
    "            for dataset in dsets:\n",
    "                if dataset == 'fam':\n",
    "                    dsets[dataset].resize((chunksize, 1 ))\n",
    "                else: \n",
    "                    dsets[dataset].resize((chunksize, np.asarray(hashesDic[dataset]).shape[0] ))\n",
    "            \n",
    "        if i % chunksize == 0 and i != 0:\n",
    "            for dataset in dsets:\n",
    "                if dataset == 'fam':\n",
    "                    dsets[dataset].resize((i+chunksize, 1 ))\n",
    "                else: \n",
    "                    dsets[dataset].resize((i+chunksize, np.asarray(hashesDic[dataset]).shape[0] ))\n",
    "\n",
    "\n",
    "        for dset in dsets:\n",
    "            if dset == 'fam':\n",
    "                dsets[dset][i,:] = fam\n",
    "            else:\n",
    "                dsets[dset][i,:] = hashesDic[dset]\n",
    "            \n",
    "h5file.close()\n",
    "#         dico_list.append(dico)\n",
    "#         # generate taxa index\n",
    "#         taxaIndex, taxaIndexReverse = profileGen.generateTaxaIndex(tree)\n",
    "\n",
    "#         # generate matrix of 1 and 0 for each biological event\n",
    "#         mat = profileGen.Tree2mat(treemap_fam, taxaIndex)\n",
    "#         mat_list.append(mat)\n",
    "\n",
    "#     except:\n",
    "#         print(\"breaking at {}\".format(fam))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "print(dsets['fam'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"gain\": shape (6, 524), type \"<i4\">"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5file = h5py.File(config.omadirLaurent + 'data1', 'a')\n",
    "\n",
    "h5file['gain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1,   0,   0,   0,   0,   0,   0,   0, 128,   0,   0,   0, 232,\n",
       "        76,  90,  55,  60, 150,   0, 156,  76, 234, 162, 215, 103, 137,\n",
       "         0, 247, 154, 220, 202,  35, 212,  42, 222, 134, 148,  18, 213,\n",
       "       126,  20,  99, 191, 102,  47, 138,  30, 159, 180,  82,  24, 183,\n",
       "       154,   1,  67, 216, 115, 239, 159,  52,  86,   5, 118, 132, 176,\n",
       "        39,  32,  99,  97, 134,  97, 124,  47, 179,  96,  93, 253,  22,\n",
       "        71, 130, 171, 178,  22,   1, 221, 238, 235,   5,  28, 155, 117,\n",
       "        12,  96, 196, 117, 174, 253, 198, 226, 105,  27,  19,  64, 218,\n",
       "        89,  26, 235,  57,  24, 246, 248,  14, 201, 120, 137, 180, 178,\n",
       "       255, 252, 113,  42,  45, 213, 113,  71, 204,  94,  36,  98, 244,\n",
       "         9, 143,  60,  52, 216,  27,   7,  82,  67, 142,   2, 130, 151,\n",
       "       183,  50,   9,  11,  40,  28,  22, 224,  92, 216,  15, 236, 252,\n",
       "       252, 230, 163, 180,  55, 104, 203,  87,  33,  23, 143, 244, 245,\n",
       "       196,  85, 157, 250, 127, 135, 230, 222,  95,  71,  57, 232,  35,\n",
       "       225, 243, 204, 114,  20,  79, 200,  85, 208, 223, 186, 244,  86,\n",
       "        43, 127,  36, 241, 217, 251,  92, 209, 162, 191,  13,  89, 206,\n",
       "        75,  92,  22,  89, 108,  46,  52,  89,  57, 122,  21, 146, 159,\n",
       "       215, 216,  78, 215, 153,  31,  71,  82,  62,  45, 209,  28,   1,\n",
       "       155,  55,  60, 203,  86,  47,  92,  76, 176, 109, 206, 205, 106,\n",
       "        20,  11, 125, 133, 158,  55,  46, 165, 221, 172, 121,  40, 223,\n",
       "         9,  19, 156, 222, 104,  77,  40, 151,  63, 231,  94, 188,  23,\n",
       "        80, 193,  48, 198, 134,  94,  92, 147,  68, 111, 237, 230, 111,\n",
       "       100, 193,  20,  46,  47, 130,  56,  38, 138, 103, 234,  17, 149,\n",
       "       109,   1,  26,  33, 174,  91,  47,   6,  13,  30, 220,  96, 219,\n",
       "        81, 143, 139,  23,  45, 203, 185, 114, 197, 142, 221,  65,  41,\n",
       "        98, 160, 227, 130, 202, 135, 225,  42, 199, 128, 116, 111, 164,\n",
       "       135,  35, 137, 204, 144, 185, 246, 128,  33,  11, 151,  27, 101,\n",
       "       246, 156,  19,  30,  53,   7,  66,  92, 104,  61, 205, 107, 153,\n",
       "       127, 197,  10, 170,  57, 163, 246, 199,  62, 131, 238, 129, 184,\n",
       "       213, 173, 243,  41, 226, 152,  43, 254,  13,  85, 122, 218,  79,\n",
       "        40,  62, 148,   9,  34, 135, 105,  86,  78,  10, 238,  13, 181,\n",
       "       144, 116,  92, 131, 128,  92,  38,  95, 242, 224, 139,  95, 120,\n",
       "         7,  64,  35,  73,  32, 220, 164, 234, 126,  42, 150, 168, 159,\n",
       "        51,  64,   8, 254, 157, 138, 168, 181,  40,  14,  64,  42, 155,\n",
       "        57, 102, 129, 118, 155, 193, 201,  19, 247, 118,  19, 186, 130,\n",
       "       117, 144, 210,  61, 208, 103, 222, 175,  56,  16,  27, 241, 135,\n",
       "        43,  39, 232, 245, 143, 255,  40, 114,  88, 150, 225, 137, 137,\n",
       "        62, 168, 243, 114,  67,  35, 215,  71, 232, 146,  64, 113, 197,\n",
       "         6, 208,   7,  80, 188, 211, 114,  78, 210,  23, 130, 228,  14,\n",
       "       210, 135,  10,  34, 110,  82,   2, 148, 143, 111, 204, 174, 112,\n",
       "       122, 113,  67,  36], dtype=int32)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5file['gain'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-9dd8862e7221>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-9dd8862e7221>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    dset = f.create_dataset(\"test\", (5,10), dtype=([(\"fam\":\"i\"), (\"duplication\":\"S128\")]))\u001b[0m\n\u001b[0m                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "f = h5py.File('hogTest.hdf5','w')\n",
    "dset = f.create_dataset(\"test\", (5,10), dtype=([(\"fam\":\"i\"), (\"duplication\":\"S128\")]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-61cdd2f5cda0>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-61cdd2f5cda0>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    dset = f.create_dataset(\"test\", (5,10), dtype([ (\"field1\": \"i\"), (\"field2\": \"f\") ]))\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dset = f.create_dataset(\"test\", (5,10), dtype([ (\"field1\": \"i\"), (\"field2\": \"f\") ]))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
