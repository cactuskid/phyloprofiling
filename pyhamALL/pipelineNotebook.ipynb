{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurent/anaconda3/lib/python3.6/site-packages/tables/leaf.py:396: PerformanceWarning: The Leaf ``/Protein/_i_Entries/OmaHOG/sorted`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
      "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
      "I/O.  You may want to reduce the rowsize by trimming the value of\n",
      "dimensions that are orthogonal (and preferably close) to the *main*\n",
      "dimension of this leave.  Alternatively, in case you have specified a\n",
      "very small/large chunksize, you may want to increase/decrease it.\n",
      "  PerformanceWarning)\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import formatOrtho\n",
    "import pyhamPipeline\n",
    "\n",
    "import pyham\n",
    "from pyoma.browser import db \n",
    "import numpy as np\n",
    "from tables import *\n",
    "import re\n",
    "from ete3 import Tree\n",
    "import pickle\n",
    "import tempfile\n",
    "import functools\n",
    "import config\n",
    "\n",
    "h5file = open_file(config.omadirLaurent + 'OmaServer.h5', mode=\"r\")\n",
    "dbObj = db.Database(h5file)\n",
    "omaIdObj = db.OmaIdMapper(dbObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_tree, replacement_dico = formatOrtho.fix_species_tree(\"speciestree.nwk\", omaIdObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho = dbObj.get_orthoxml(100000).decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "newortho = formatOrtho.convert_orthoxml_ids(ortho, replacement_dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "hamObj = pyham.Ham(species_tree, newortho, type_hog_file=\"string\", use_internal_name = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'100000': <HOG(100000)>}\n"
     ]
    }
   ],
   "source": [
    "print(hamObj.get_dict_top_level_hogs())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog21 = hamObj.get_hog_by_id('100000')\n",
    "tp = hamObj.create_tree_profile(hog=hog21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tree node 'rosids' (-0x7ffff80dd97ec1c4)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tp.treemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fabids\n",
      "N\n",
      "1\n",
      "D\n",
      "0\n",
      "L\n",
      "0\n",
      "I\n",
      "1\n",
      "malvids\n",
      "N\n",
      "1\n",
      "D\n",
      "0\n",
      "L\n",
      "0\n",
      "I\n",
      "1\n",
      "Vitis vinifera\n",
      "N\n",
      "0\n",
      "D\n",
      "0\n",
      "L\n",
      "1\n",
      "I\n",
      "0\n",
      "Papilionoideae\n",
      "N\n",
      "1\n",
      "D\n",
      "0\n",
      "L\n",
      "0\n",
      "I\n",
      "1\n",
      "Malpighiales\n",
      "N\n",
      "0\n",
      "D\n",
      "0\n",
      "L\n",
      "1\n",
      "I\n",
      "0\n",
      "Prunus persica\n",
      "N\n",
      "0\n",
      "D\n",
      "0\n",
      "L\n",
      "1\n",
      "I\n",
      "0\n",
      "Brassicaceae\n",
      "N\n",
      "1\n",
      "D\n",
      "0\n",
      "L\n",
      "0\n",
      "I\n",
      "1\n",
      "Malvaceae\n",
      "N\n",
      "0\n",
      "D\n",
      "0\n",
      "L\n",
      "1\n",
      "I\n",
      "0\n",
      "Lotus japonicus\n",
      "N\n",
      "0\n",
      "D\n",
      "0\n",
      "L\n",
      "1\n",
      "I\n",
      "0\n",
      "Glycine max\n",
      "N\n",
      "0\n",
      "D\n",
      "0\n",
      "L\n",
      "1\n",
      "I\n",
      "0\n",
      "Medicago truncatula\n",
      "N\n",
      "1\n",
      "D\n",
      "0\n",
      "L\n",
      "0\n",
      "I\n",
      "1\n",
      "Manihot esculenta\n",
      "N\n",
      "0\n",
      "D\n",
      "0\n",
      "L\n",
      "0\n",
      "I\n",
      "0\n",
      "Populus trichocarpa\n",
      "N\n",
      "0\n",
      "D\n",
      "0\n",
      "L\n",
      "0\n",
      "I\n",
      "0\n",
      "Brassica\n",
      "N\n",
      "1\n",
      "D\n",
      "0\n",
      "L\n",
      "0\n",
      "I\n",
      "1\n",
      "Arabidopsis\n",
      "N\n",
      "0\n",
      "D\n",
      "0\n",
      "L\n",
      "1\n",
      "I\n",
      "0\n",
      "Theobroma cacao\n",
      "N\n",
      "0\n",
      "D\n",
      "0\n",
      "L\n",
      "0\n",
      "I\n",
      "0\n",
      "Gossypium hirsutum\n",
      "N\n",
      "0\n",
      "D\n",
      "0\n",
      "L\n",
      "0\n",
      "I\n",
      "0\n",
      "Brassica napus\n",
      "N\n",
      "1\n",
      "D\n",
      "0\n",
      "L\n",
      "0\n",
      "I\n",
      "1\n",
      "Brassica oleracea\n",
      "N\n",
      "1\n",
      "D\n",
      "0\n",
      "L\n",
      "0\n",
      "I\n",
      "1\n",
      "Arabidopsis lyrata\n",
      "N\n",
      "0\n",
      "D\n",
      "0\n",
      "L\n",
      "0\n",
      "I\n",
      "0\n",
      "Arabidopsis thaliana\n",
      "N\n",
      "0\n",
      "D\n",
      "0\n",
      "L\n",
      "0\n",
      "I\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for node in tp.treemap.traverse():\n",
    "    if not node.is_root():\n",
    "        print(node.name)\n",
    "        print('N')\n",
    "        print(node.nbr_genes)\n",
    "        print('D')\n",
    "        print(node.dupl)\n",
    "        print('L')\n",
    "        print(node.lost)\n",
    "        print('I')\n",
    "        print(node.identical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = Tree(species_tree , format =1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {}\n",
    "mappingINV = {}\n",
    "for i,node in enumerate(T.traverse()):\n",
    "    mapping[i] = node.name\n",
    "    mappingINV[node.name] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasketch\n",
    "import ete3\n",
    "import sparse\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fabids\n",
      "malvids\n",
      "Vitis vinifera\n",
      "Papilionoideae\n",
      "Malpighiales\n",
      "Prunus persica\n",
      "Brassicaceae\n",
      "Malvaceae\n",
      "Lotus japonicus\n",
      "Glycine max\n",
      "Medicago truncatula\n",
      "Manihot esculenta\n",
      "Populus trichocarpa\n",
      "Brassica\n",
      "Arabidopsis\n",
      "Theobroma cacao\n",
      "Gossypium hirsutum\n",
      "Brassica napus\n",
      "Brassica oleracea\n",
      "Arabidopsis lyrata\n",
      "Arabidopsis thaliana\n",
      "(15, 524)\n",
      "[[  1   0   0 ... 174 193  24]\n",
      " [  1   0   0 ...  79  24 238]\n",
      " [  1   0   0 ...  53  76 106]\n",
      " ...\n",
      " [  1   0   0 ... 174 193  24]\n",
      " [  1   0   0 ...  53  76 106]\n",
      " [  1   0   0 ... 174 193  24]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def Tree2Hashes(treemap):\n",
    "    #turn each tree into a minhash object\n",
    "    #serialize and store as array\n",
    "    eventdict = { 'presence':[] , 'gain':[] , 'loss':[] , 'duplication':[]}\t\n",
    "\n",
    "    for node in treemap.traverse():\n",
    "\n",
    "        if not node.is_root():\n",
    "            print(node.name)\n",
    "            if node.nbr_genes >0:\n",
    "                eventdict['presence'].append('P'+node.name)\n",
    "            if node.dupl > 0:\n",
    "                eventdict['duplication'].append('D'+node.name)\n",
    "            if node.lost > 0:\n",
    "                eventdict['loss'].append('L'+node.name)\n",
    "        else:\n",
    "            eventdict['gain'].append('G'+node.name)\n",
    "\n",
    "    hashes = []\n",
    "\n",
    "    hashesDict = {}\n",
    "\n",
    "    for array in eventdict:\n",
    "        eventdict[array] = set(eventdict[array])\n",
    "\n",
    "        minHash = datasketch.MinHash(num_perm=128)\n",
    "\n",
    "        for element in eventdict[array]:\n",
    "\n",
    "            minHash.update(element.encode())\n",
    "\n",
    "        hashesDict[array] = minHash\n",
    "\n",
    "        lminHash = datasketch.LeanMinHash(minHash)\n",
    "\n",
    "        buf = bytearray(lminHash.bytesize())\n",
    "        lminHash.serialize(buf)\n",
    "        hashes.append([buf])\n",
    "\n",
    "    for j in range(1,len(eventdict.keys())):\n",
    "        for i in itertools.combinations(eventdict.keys(), j+1):\n",
    "            combName = ''\n",
    "            minHash = datasketch.MinHash(num_perm=128)\n",
    "            for array in i:\n",
    "                combName += array\n",
    "                minHash.merge(hashesDict[array])\n",
    "\n",
    "            hashesDict[combName] = minHash\n",
    "\n",
    "            lminHash = datasketch.LeanMinHash(minHash)\n",
    "\n",
    "            buf = bytearray(lminHash.bytesize())\n",
    "            lminHash.serialize(buf)\n",
    "            hashes.append([buf])\n",
    "\n",
    "    hashmat = np.vstack(hashes)\n",
    "    return hashmat, hashesDict\n",
    "\n",
    "\n",
    "hashmat, hashesDict = Tree2Hashes(tp.treemap)\n",
    "print(hashmat.shape)\n",
    "print(hashmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'duplication': <datasketch.minhash.MinHash at 0x7f228c9db0f0>,\n",
       " 'gain': <datasketch.minhash.MinHash at 0x7f228c9dbfd0>,\n",
       " 'gainduplication': <datasketch.minhash.MinHash at 0x7f228c9db160>,\n",
       " 'gainloss': <datasketch.minhash.MinHash at 0x7f228c9db4e0>,\n",
       " 'gainlossduplication': <datasketch.minhash.MinHash at 0x7f228c9db710>,\n",
       " 'loss': <datasketch.minhash.MinHash at 0x7f228c9db2b0>,\n",
       " 'lossduplication': <datasketch.minhash.MinHash at 0x7f228c9dbba8>,\n",
       " 'presence': <datasketch.minhash.MinHash at 0x7f228c9dbda0>,\n",
       " 'presenceduplication': <datasketch.minhash.MinHash at 0x7f228c9dbe80>,\n",
       " 'presencegain': <datasketch.minhash.MinHash at 0x7f228c9dbc50>,\n",
       " 'presencegainduplication': <datasketch.minhash.MinHash at 0x7f228c9dba90>,\n",
       " 'presencegainloss': <datasketch.minhash.MinHash at 0x7f228c9dbac8>,\n",
       " 'presencegainlossduplication': <datasketch.minhash.MinHash at 0x7f228c9db748>,\n",
       " 'presenceloss': <datasketch.minhash.MinHash at 0x7f228c9dbb70>,\n",
       " 'presencelossduplication': <datasketch.minhash.MinHash at 0x7f228c9dbcf8>}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashesDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurent/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py:742: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "def Tree2mat(eteobj,taxaIndex):\n",
    "    #use partials to configure the taxa index\n",
    "    #turn each tree into a sparse matrix with 4 rows\n",
    "    rowdict={ 'presence':0 , 'gain':1 , 'loss':2 , 'duplication':3}\n",
    "    \n",
    "    mat = csr_matrix( (len(rowdict), len(taxaIndex) ) )\n",
    "    \n",
    "    coords = []\n",
    "    for node in eteobj.traverse():\n",
    "    # traverse() returns an iterator to traverse the tree structure\n",
    "    # strategy:\"levelorder\" by default; nodes are visited in order from root to leaves\n",
    "    # it return treeNode instances\n",
    "        if not node.is_root():            \n",
    "            if node.nbr_genes >0:\n",
    "                mat[ rowdict['presence'] , taxaIndex[node.name]] =1 \n",
    "            if node.lost > 0:\n",
    "                mat[ rowdict['loss'] , taxaIndex[node.name]] =1\n",
    "            if node.dupl > 0:\n",
    "                mat[ rowdict['duplication'] , taxaIndex[node.name]] =1\n",
    "        else:\n",
    "            mat[ rowdict['gain'] , taxaIndex[node.name]] =1\n",
    "  \n",
    "    return mat\n",
    "\n",
    "print(np.sum(Tree2mat(tp.treemap, mappingINV)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurent/anaconda3/lib/python3.6/site-packages/tables/leaf.py:396: PerformanceWarning: The Leaf ``/Protein/_i_Entries/OmaHOG/sorted`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
      "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
      "I/O.  You may want to reduce the rowsize by trimming the value of\n",
      "dimensions that are orthogonal (and preferably close) to the *main*\n",
      "dimension of this leave.  Alternatively, in case you have specified a\n",
      "very small/large chunksize, you may want to increase/decrease it.\n",
      "  PerformanceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  <species name=\"Haloredivivus sp. (strain G17)\" NCBITaxId=\"1072681\">\n",
      "['1873']\n",
      "<?xml version='1.0' encoding='UTF-8'?>\n",
      "<orthoXML xmlns=\"http://orthoXML.org/2011/\" origin=\"OMA\" originVersion=\"Nov 2017\" version=\"0.3\">\n",
      "  <species name=\"Nanosalinarum_sp___strain_J07AB56_\" NCBITaxId=\"889962\">\n",
      "    <database name=\"Candidatus Nanosalinarum sp. J07AB56 (hypersaline lake metagenome)\" version=\"GL982569.1  GI:339756006\">\n",
      "      <genes>\n",
      "        <gene id=\"6851\" protId=\"NANSJ01348\"/>\n",
      "      </genes>\n",
      "    </database>\n",
      "  </species>\n",
      "  <species name=\"Nanosalina_sp___strain_J07AB43_\" NCBITaxId=\"889948\">\n",
      "    <database name=\"Candidatus Nanosalina sp. J07AB43 (hypersaline lake metagenome)\" version=\"GL982576.1  GI:339757416\">\n",
      "      <genes>\n",
      "        <gene id=\"5465\" protId=\"NANS001635\"/>\n",
      "      </genes>\n",
      "    </database>\n",
      "  </species>\n",
      "  <groups>\n",
      "    <orthologGroup id=\"1\">\n",
      "      <property name=\"TaxRange\" value=\"Nanohaloarchaea\"/>\n",
      "      <geneRef id=\"5465\"/>\n",
      "      <geneRef id=\"6851\"/>\n",
      "    </orthologGroup>\n",
      "  </groups>\n",
      "</orthoXML>\n",
      "\n",
      "\n",
      "['1873']\n",
      "1\n",
      "  <species name=\"Haloredivivus sp. (strain G17)\" NCBITaxId=\"1072681\">\n",
      "['2598']\n",
      "<?xml version='1.0' encoding='UTF-8'?>\n",
      "<orthoXML xmlns=\"http://orthoXML.org/2011/\" origin=\"OMA\" originVersion=\"Nov 2017\" version=\"0.3\">\n",
      "  <species name=\"Nanosalina_sp___strain_J07AB43_\" NCBITaxId=\"889948\">\n",
      "    <database name=\"Candidatus Nanosalina sp. J07AB43 (hypersaline lake metagenome)\" version=\"GL982576.1  GI:339757416\">\n",
      "      <genes>\n",
      "        <gene id=\"4023\" protId=\"NANS000193\"/>\n",
      "      </genes>\n",
      "    </database>\n",
      "  </species>\n",
      "  <groups>\n",
      "    <orthologGroup id=\"2\">\n",
      "      <property name=\"TaxRange\" value=\"Nanohaloarchaea\"/>\n",
      "      <geneRef id=\"4023\"/>\n",
      "    </orthologGroup>\n",
      "  </groups>\n",
      "</orthoXML>\n",
      "\n",
      "\n",
      "['2598']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/laurent/anaconda3/lib/python3.6/site-packages/scipy/sparse/compressed.py:742: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  SparseEfficiencyWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Minimum 2 genomes are required, only 1 provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-dee44aacefb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mortho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformatOrtho\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_orthoxml_ids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mortho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacement_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mortho\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mham_fam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyhamPipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ham\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdbObj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspecies_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplacement_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# generate tree profile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/phylo/phyloprofiling/pyhamALL/pyhamPipeline.py\u001b[0m in \u001b[0;36mget_ham\u001b[0;34m(fam, dbObj, species_tree, replacement_dic, datadir, l)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# get ham Object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mhamObj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyham\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspecies_tree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mortho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_hog_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"string\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_internal_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhamObj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyham/ham.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, newick_str, hog_file, type_hog_file, filter_object, use_internal_name)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhog_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                 \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_level_hogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextant_gene_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternal_id_mapper\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mfactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoplevel_hogs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextant_gene_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternal_id_mapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyham/parsers.py\u001b[0m in \u001b[0;36mend\u001b[0;34m(self, tag)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;31m# get the ancestral genome related to this hog based on it's children\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m                 \u001b[0mancestral_genome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mham_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ancestral_genome_by_mrca_of_hog_children_genomes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m                 \u001b[0mhog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_genome\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mancestral_genome\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0mancestral_genome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtaxon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_gene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyham/ham.py\u001b[0m in \u001b[0;36m_get_ancestral_genome_by_mrca_of_hog_children_genomes\u001b[0;34m(self, hog)\u001b[0m\n\u001b[1;32m    826\u001b[0m         \u001b[0mchildren_genomes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenome\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_ancestral_genome_by_mrca_of_genome_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchildren_genomes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_ancestral_genome_by_mrca_of_genome_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenome_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pyham/ham.py\u001b[0m in \u001b[0;36m_get_ancestral_genome_by_mrca_of_genome_set\u001b[0;34m(self, genome_set)\u001b[0m\n\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenome_set\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Minimum 2 genomes are required, only {} provided.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenome_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgenome_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Minimum 2 genomes are required, only 1 provided."
     ]
    }
   ],
   "source": [
    "import pyham\n",
    "from pyoma.browser import db \n",
    "import numpy as np\n",
    "from tables import *\n",
    "import re\n",
    "from ete3 import Tree\n",
    "import pickle\n",
    "import tempfile\n",
    "import functools\n",
    "import config\n",
    "\n",
    "import pyhamPipeline\n",
    "import profileGen\n",
    "\n",
    "# with open( config.working_dir + \"speciestree.nwk\" , 'r') as treefile:\n",
    "#     species_tree = treefile.read()\n",
    "# t = Tree(species_tree, format =1 )\n",
    "# print(t)\n",
    "# #rewrite IDs and reload tree\n",
    "# #\n",
    "# species_tree = pyham.utils.get_newick_string(working_dir + \"speciestree_hack.nwk\", type=\"nwk\")\n",
    "\n",
    "\n",
    "#open up OMA\n",
    "h5file = open_file(config.omadirLaurent + 'OmaServer.h5', mode=\"r\") \n",
    "#setup db objects\n",
    "dbObj = db.Database(h5file)\n",
    "omaIdObj = db.OmaIdMapper(dbObj)\n",
    "\n",
    "# corrects species tree and replacement dictionary for orthoXML files\n",
    "species_tree, replacement_dic = formatOrtho.fix_species_tree(\"speciestree.nwk\", omaIdObj)\n",
    "\n",
    "#temporary, \n",
    "hashmat_list = [] \n",
    "mat_list = []\n",
    "\n",
    "#load Fam\n",
    "for fam in pyhamPipeline.yieldFamilies(h5file):\n",
    "\n",
    "    # generate pyham object\n",
    "#     print(fam)\n",
    "    ortho = dbObj.get_orthoxml(fam).decode()\n",
    "#     print(ortho)\n",
    "    ortho = formatOrtho.convert_orthoxml_ids(ortho, replacement_dic, verbose = True)\n",
    "    print(ortho)\n",
    "    ham_fam = pyhamPipeline.get_ham(fam, dbObj, species_tree, replacement_dic)\n",
    "\n",
    "    # generate tree profile\n",
    "    treemap_fam = pyhamPipeline.pyhamtoTree(ham_fam, fam)\n",
    "\n",
    "    # generate matrix of hash\n",
    "    hashmat = profileGen.Tree2Hashes(treemap_fam)\n",
    "    hashmat_list.append(hashmat)\n",
    "    # generate taxa index\n",
    "    taxaIndex, taxaIndexReverse = profileGen.generateTaxaIndex(species_tree)\n",
    "\n",
    "    # generate matrix of 1 and 0 for each biological event\n",
    "    mat = profileGen.Tree2mat(treemap_fam, taxaIndex)\n",
    "    mat_list.append(mat)\n",
    "    print(fam)\n",
    "\n",
    "    \n",
    "    if fam > 150:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
